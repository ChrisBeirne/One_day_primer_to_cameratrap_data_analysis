# Analysis covariates {#covariates}

Once we have created the building blocks for our data analysis dataframes, we must bring in the variables which will be used in the modelling steps. It is important to not that there are millions of ways to add covariates - both in terms of how you do it, and where you derive the data from. The covariates you use will depend on the questions you have, and the context of your survey. The examples provided here are not comprehensive and serve only as a guide!

**Create a new .R script**

Call it `02_example_covariates.R`.

**Load the required packages and read in the locations dataframe**

```{r ch4_1, echo=T, results='hide', message =F, warning=F, class.source="Rmain"}
library(kableExtra);library(dplyr); library(sf); library(MODISTools); library(lubridate); library(corrplot); library(terra); library(osmdata); library(elevatr)

# Locations data frame 
locs <- read.csv("data/processed_data/AlgarRestorationProject_camera_locations.csv", header=T)

```

## Camera station covariates

It is common to have a suite of covariates which you would like to investigate the effects of in your datasets. These could take the form of habitat designations or treatment types. These may already be included with your deployment data, or you may need to derive them from a variety of remote sources. In their simplest form, these variable are time invariant (they do not change), however you may have variables which change through time as well (we discuss these at the end). In the following steps, we walk through the process of manipulating and deriving example covariates.

For the time invariant covariates, we will add them to our `locs` dataframe imported above.

### Locally collected covariates

You may have collected some data in the field when deploying or checking your camera traps, and kept that data separate from your camera trap data (e.g. vegetation assessments). Provided that the naming convention you gave to these dataframes is the same as in your camera data (e.g. the location is in a column called `placename`) - you can do a 'left_join()` to merge the two datasets.

Import a sample set of local covariates and look at it:

```{r ch4_11, class.source="Rmain"}
# Import
local_covs <- read.csv("data/raw_data/example_covariates/example_dataframe.csv")

# Add it to you locations datafile
locs <- left_join(locs, local_covs)   # From the dplyr package

# Look at the data
local_covs %>% kbl() %>% 
  kableExtra::scroll_box(height = "250px")

```

It is a dataframe where the survey locations are rows and the local covariates, in this case `line_of_sight_m`, are columns.

We added this data to our station data using a `left_join()` operation from the `dplyr()` package. It uses a key variable which is common in both data frames to add data from the "right-hand side" to the rows in the "left-hand side" which are not already present. Any rows present in the right-hand side which are not in the left-hand side will be skipped.

For more examples of joins using `dplyr()` see: [https://dplyr.tidyverse.org/reference/mutate-joins.html](https://dplyr.tidyverse.org/reference/mutate-joins.html)

### Remotely collected covariates

To exploit remotely collected data sources we need to use the `sf` package to help us with spatial data.

Lets convert our "normal" dataframe to an `sf` dataframe:

```{r ch4_14, class.source="Rmain"}
# Convert
locs_sf <- st_as_sf(locs,                              # We specify the dataframe 
                    coords=c("longitude", "latitude"), # The XY coordinates
                    crs=4326)                          # And the projection code

# Look
locs_sf


```

That weird header is important - it tells you the type of data you have (lines, points, polygons etc), and the projection information (`CRS`).

For more in depth information of `sf` functionality see: [https://r-spatial.github.io/sf/articles/sf1.html](https://r-spatial.github.io/sf/articles/sf1.html) 


### Extracting data from local rasters

Often we have raster data layers stored which we would like to link to our camera locations. We have included one such example here, a raster which reflects the depth from the soil surface to the water table - a proxy for habitat type in this study site. The layer comes from the 1m Wet Area Mapping (WAM) layer:

[White, Barry, et al. "Using the cartographic depth-to-water index to locate small streams and associated wet areas across landscapes." Canadian Water Resources Journal 37.4 (2012): 333-347.](https://www.tandfonline.com/doi/full/10.4296/cwrj2011-909)

*NOTE* the raster has been down scaled to reduce its size for this course - it is no longer at 1m resolution.

The only time we deviate from the `sf` package is to deal with rasters. Raster objects in R are processed really slowly, especially if the raster is large. So instead we use the `terra` package.

```{r  ch4_19, class.source="Rmain"}
library(terra)

# Import the example raster using the stars package
ras <- rast("data/raw_data/example_covariates/example_raster.tif")
# Covert your sf locations to the same projection as your raster then put it in terra `vect` format,
locs_terra <- locs_sf %>% 
                st_transform(crs=st_crs(ras)) %>% # change the projection to match the raster
                vect() # Turn it into a terra object

# Plot the result
plot(ras) # The terra package makes nice raster plots with legends
plot(locs_terra, add=T) # Add the survey locations as black dots


```

Great! Now lets buffer our camera locations by 250 meters, and take the average depth to water for each location:

```{r ch4_21, class.source="Rmain"}
# Buffer by 250m
locs_terra <- buffer(locs_terra,250)

# Extract the values to a temporary object - tmp 
tmp <- raster::extract(ras, locs_terra, fun=mean)

# Make a new column in locs_sf called water_depth_m
# They are ordered the same way so no need for a fancy join
locs_sf$water_depth_m <- tmp$Depth2WatAlgar

# Check the distribution of our data
boxplot(locs_sf$water_depth_m)

```

Most locations are on the water table (lowland sites), others are above it (upload sites), and they have different vegetation characteristics in the field. 

### elevatr package

Camera studies often occur over a range of elevations - and we can quickly extract these elevations using the `elevatr` package and an `sf` dataframe.

The `src` option specifies the sources of the DEM data. We use `aws` Amazon Web Service Terrain Tiles - which are available globally.

The `z` option specifies the resolution of the underlying DEM, the high the value, the more detailed it is. However, it will take longer to run so do not go crazy. 

```{r ch4_23, class.source="Rmain", warning=F, message=F}
library(elevatr)
locs_sf <- get_elev_point(locs_sf, 
                          src="aws", #Amazon Web Service Terrain Tiles - available globally
                          z = 12)  # z specifies the zoom level, the lower the value the faster the code runs, but the coarser the elevation values are

# Plot the result
boxplot(locs_sf$elevation)


```

An elevation of ~ 500m was expected. Great!

If you want to download a full elevation raster for your area of interests, see the [`introduction to elevatr`](https://cran.r-project.org/web/packages/elevatr/vignettes/introduction_to_elevatr.html)


### Open Street Maps
Open Street Map (OSM) is an incredible resource for generating covariates for camera trap studies. For example, we might be interested in the distance to the nearest rivers, roads, or trails. All of these anthropogenic features are available in OSM!

**CAREFUL** OSM data is user contributed and often incomplete and patchy. Always plot your data and never assume it is complete without checking it first. For an example fo this see `water bodies` below. 

First lets load the `osmdata` package.

```{r ch4_25, class.source="Rmain"}
library(osmdata)
```

The types of features we can extract using the `osmdata` package are listed here: [https://wiki.openstreetmap.org/wiki/Map_features](https://wiki.openstreetmap.org/wiki/Map_features).

#### Highways

Camera trap projects are often interested in human disturbance, of which, highways are an important part.

Let's start by defining our area of interest. All `osmdata` queries begin with a bounding box defining the area of the query:  

```{r ch4_26, class.source="Rmain"}
# First buffer our points by 10km to create an area of interest (aoi)
aoi <- st_bbox(st_buffer(locs_sf, 10000)) # Units are in meters 

# Return the features inside the bounding box
highway <- opq(aoi) %>% #using the bounding box
           add_osm_feature(key="highway") %>% #extract all highway features
           osmdata_sf()  # convert them into simple features format


```

The data you extract is its own "class" of data made up from multiple data types. The key thing is that it is made up of multiple data slices, each of which represents an `sf` dataset. Let's take a look at three of these
- $osm_points
- $osm_lines
- $osm_polygons

Let's use the lines element and add our camera stations, then calculate the distances to them:

```{r ch4_30, class.source="Rinfo"}
# Create an index of the nearest object in `highway$osm_lines` to locs_sf
index <- st_nearest_feature(locs_sf, highway$osm_lines)

# Use that index to ask for the distance to that object
locs_sf$road_dist_m <- st_distance(locs_sf, highway$osm_lines[index,], 
                                   by_element=T) # Note `by_element=T` tells st_distance to evaluate things line by line. 

# Plot the data
par(mfrow=c(1,1))
plot(st_as_sfc(aoi))     # st_as_sfc created a polygon from a `bbox` object
plot(st_geometry(highway$osm_lines), add=T)
plot(st_geometry(locs_sf), col="red", add=T)

```

`st_nearest_feature` gives us the index number of the feature which is closest to each station. 

We can the use this to request the distance from that nearest feature to each camera station using `st_distance`. Which, put together, looks like:


#### water bodies

We also might want to calculate the distances to the nearest water body, and important resource for wildlife. We can do that using the following: 

```{r ch4_32, class.source="Rmain"}
water <- opq(aoi) %>%
           add_osm_feature(key="water") %>%
           osmdata_sf()

# This time we will use the points file
index <- st_nearest_feature(locs_sf, water$osm_points)

locs_sf$water_dist_m <- st_distance(locs_sf, water$osm_points[index,], by_element=T) # Note `by_element=T` tells st_distance to evaluate things line by line. 

```

For more examples of using the `osmdata` package see: [the projects github page](https://github.com/ropensci/osmdata)

### Vegetation productivity

#### MODISTools 

`MODIStools` is an R interface to the [MODIS Land Products Subsets](https://modis.ornl.gov/data/modis_webservice.html) web services. It allows for easy access to ‘MODIS’ time series directly to your computer! These are the data layers commonly used to extract normalized difference vegetation index (NDVI) and Enhanced Vegetation Index (EVI) information. When using `MODIStools` you should reference: 

[Hufkens (2022). The MODISTools package: an interface to the MODIS Land Products Subsets Web Services](https://github.com/ropensci/MODISTools) 

Also click that link for more details on how to use it. 

Two commonly data products are [MOD13Q1](https://lpdaac.usgs.gov/products/mod13q1v006/) for the derivation of NDVI/EVI, and [MOD15A2H](https://lpdaac.usgs.gov/products/mod15a2hv006/) for the derivation of leaf area index (LAI).

Let's load the package and get it into the right format:

```{r ch4_46, class.source="Rmain"}
library(MODISTools)

# Select the location data and put it into the format MODIS tools expects
modis_locs <- locs %>% 
  select("placename", "longitude", "latitude") %>% 
  rename(site_name=placename, lat=latitude, lon=longitude)

# Download some NDVI data
site_ndvi <- mt_batch_subset(product = "MOD13Q1",
                              df=modis_locs,
                              band = "250m_16_days_NDVI",
                              start = "2019-07-01",
                              end = "2019-08-31",
                              km_lr = 0,         # Use these options if you want to buffer the value (km left)
                              km_ab = 0,         # Use these options if you want to buffer the value (km above)
                              internal = TRUE)

```


Lets simplify the output to the key elements of information and rename them to match our camera data where appropriate:

```{r ch4_52, class.source="Rmain", message=F, warning=F}
# Reduce the number of columsn in the output and rename it where required.
ndvi_simple <- site_ndvi %>% 
  select(	site, band, calendar_date, value) %>% 
  rename(placename=site)

# Take the average for each location
tmp <- ndvi_simple %>%             #Take the NDVI layer
  group_by(placename) %>%          # Group observations by the placename
  summarize(mean_ndvi=mean(value)) # Take the mean of the values and call the new column `mean_ndvi`

# Add the new data to our locations dataframe
locs_sf <- left_join(locs_sf, tmp)

# Plot the results
boxplot(locs_sf$mean_ndvi,
        ylab="Mean NDVI score",
        las=1)


```

It is possible to generate an NDVI score for each month that each camera is active, however that would take too long to produce for this course! 

## Convert and save your covariates

```{r ch4_57, message=F, warning=F, class.source="Rmain"}
# Convert columns to numeric
locs_sf$road_dist_m <- as.numeric(locs_sf$road_dist_m)

# Convert it back to a dataframe
locs_sf$geometry <- NULL

locs <- left_join(locs, locs_sf)


# Write the dataset

write.csv(locs, paste0("data/processed_data/", locs$project_id[1],"_camera_locations_and_covariates.csv"), row.names=F)

```


## Correlations between predictors 

So we have used a variety of different techniques to generate covariates for our subsequent analyses. However, it is important to note that we cannot just through these variables into a model.

One way to check if your different variables are confound/correlated is using the `corrplot` package.


```{r ch4_58, class.source="Rmain"}
library(corrplot)

# First we need to create a correlation matrix between the different variables of interest
M <- cor(locs[, c("line_of_sight_m", "water_depth_m", "elevation",
                     "road_dist_m", "mean_ndvi")])

corrplot(M,                              #The correlation matrix we made
         method="color",                 # How we want the cells 
         type="upper",                   # Just show the upper part (it is usually mirrored)
         order="hclust",                 # Order the variables using the hclust method
         addCoef.col = "black",          # Add coefficient of correlation  
         tl.col="black", tl.srt=45,      # Control the text label color and rotation
         diag=F                          # Suppress the diagonal correlations (which are 1 anyway)
         )

```

The cells denote pairwise correlations between the rows and the columns. The great thing about `corrplot` is customization option are near endless - see [the corrplot vignette](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html).

In general there is very low correlation between our different predictors! If we were seeing pairwise correlations >0.7 we perhaps wouldn't include those in the same model. 




