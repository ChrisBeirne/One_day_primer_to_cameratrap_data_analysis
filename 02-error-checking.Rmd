# Error checking {#error-checking}

The most important part of analyzing camera trap data is checking it! 

Dynamic data exploration is key - check your data as you go. If you leave it to the end of your data collection and processing, the opportunity to correct mistakes early will be lost! On the projects I am currently working on this means downloaded the current dataset once a month or so, and checking that 'everything' looks good. But what is 'everything'?

## Standardised exploration script

In the Wildlife Coexistence lab we use a standardized R script to check the data generated by camera trap projects. 

The most up-to-date script for exploring a single site is available on our [GitHub page](https://github.com/WildCoLab/SingleSiteExploration).

Below we run through the key plots and outputs this script generates.

The first step is to read in your data:

```{r}
# Load your data 
pro <- read.csv("data/raw_data/example_data/proj.csv", header=T)
img <- read.csv("data/raw_data/example_data/img.csv", header=T)
dep <- read.csv("data/raw_data/example_data/dep.csv", header=T)
cam <- read.csv("data/raw_data/example_data/cam.csv", header=T)

```

Next, load in the packages we will use in this chapter:

```{r, echo=T, eval=F}
#Load Packages
list.of.packages <- c("leaflet", "dplyr", "viridis", "kriging", "corrplot", "lubridate", "kableExtra", "tidyr", "taxize", "plotly", "sf")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages,repos = "http://cran.us.r-project.org")
lapply(list.of.packages, require, character.only = TRUE)

```


```{r, include=F}
#Load Packages
list.of.packages <- c("leaflet", "dplyr", "viridis", "kriging", "corrplot", "lubridate", "kableExtra", "tidyr", "taxize", "plotly", "sf")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages,repos = "http://cran.us.r-project.org")
lapply(list.of.packages, require, character.only = TRUE)

```


## Format your dates

Dealing with date objects in R can be intimidating, but once you get the hang of it, it is incredibly powerful. The process has been made far easier with the 'lubridate' package. 

The first dates we need to convert are those in the `dep` datasheet - the start and end times of each deployment. The way 'lubridate' works is you specify the order of the days, months years, hours, minutes and seconds with the codes d,m,y,h,m, and s respectively. 

### Lubridate examples 
Try importing the 25th of December in a couple of formats:

```{r}
#library(lubridate)
# day-month-year
dmy("24-12-2022")

# year-month-day
ymd("2022-12-24")
```

The output should be identical. Note `lubridate` defaults to UTC - unless otherwise specified. 

Now the real power of lubridate lies in the fact that you can handle multiple different date formats in one column using the  `parse_date_time()` function. This sometimes happens - I usually blame excel - but you could be merging two datasets formatted in different ways too. Lets try it:

```{r}
x <- c("24-12-2022", "2022-12-24", "12-24-2022")
parse_date_time(x, c("ymd", "dmy", "mdy"))
```

They should give all the same output!

Next, lets calculate the amount of time which has elapsed between two dates. A fundamental operation in the managment of camera data. To do this we first create an interval object `interval(date1, date2)`, then ask to return the object in days `/ddays(1)`. Lets try it:

```{r, eval=F}
date1 <- ymd("2021-10-13")
date2 <- ymd("2021-12-11")

interval(date1, date2)/ddays(1)
```

How many days elapsed between those two dates?

### Deployment dates

Lets get back to the camera data. Which lubridate format should we use for `r dep$start_date[1]`?

`ymd()` should do the job.

```{r}
# start dates
dep$start_date <- ymd(dep$start_date)

# end dates
dep$end_date   <- ymd(dep$end_date)
```

Now lets make a new colum in the deployment data called `days`, and calcuklate the interval for all the deployments:

```{r}
dep$days <- interval(dep$start_date, dep$end_date)/ddays(1)
```

What are the range of dates the deployments were active for? Things to look out for are 0's, NA's and negative numbers. 

```{r, eval=F}
summary(dep$days)
```

**What does a values of zero mean?** Lets look at it:

```{r}
dep[dep$days==0,]
```

They are deployments which started and ended on the same day -> they must have failed. 

**what does an NA mean?**
```{r}
dep[is.na(dep$days)==T,]
```

This deployment does not even have an end date, it must have been stolen!

**What would a negative number mean?** 

Someone probably got the start and end date the wrong way round. It happens! Go back and check your datasheets. 

### Image dates
We next need to convert the `timestamp` column in the image dataframe. What `lubridate` format is required for a a date which looks like `r img$timestamp[1]`?

```{r}
ymd_hms("2015-11-21 03:03:44")
```

Lets apply this to our image dataset:

```{r}
img$timestamp <- ymd_hms(img$timestamp)
```

If you want to learn more about the amazing functionality of the 'lubridate' package - check out the pdf [Lubridate Cheatsheet](https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf)


```{r non-adjustable options, echo=F, include=F}

# Count the number of camera ststions
n.stat <- length(unique(dep$deployment_id))



# Generate colours to display the catagory levels - R needs them as a factor
#sta[,category] <- factor(sta[,category])
#col.cat <- turbo(length(levels(sta[,category])))
#dep$Cols <- col.cat[sta[,category]]

# # How big should the figures be
# dep.height <- 8
# if(length(unique(dep$deployment_id))>80)
#    {
#      dep.height <- length(unique(dep$deployment_id))/10
#    }
# 
# sp.height <- 7
# if(length(unique(dat$Species))>20)
#    {
#      sp.height <- 7+(length(unique(dat$Species))/8)
#    }


```

### Basic summaries

Now that our camera trap data are loaded into R, we can very quickly find out summary information about the dataset:

```{r}
# Number of unique locations - we assign this as a object as we use it elsewhere
 n.stat

# Number of records
nrow(img)

# Start and end date of the project
min(dep$start_date)
max(dep$end_date)

# Number of Blanks [NOT YET DONE]
#`r nrow(dat[dat$Blank==FALSE,])` are classified as blanks (`r round((nrow(dat[dat$Blank==TRUE,])/nrow(dat))*100,1)`% of the total data set). Of the detections which have been identified, there are `r length(levels(factor(dat$Species)))` different categories. 


```

### Quick error checks

There are some error checks which are so common that we want to get them out of the way right off the bat. If the dataset fails the following tests, then the subsequent scripts will definitely not run:


#### Camera locations

A common mistake in camera trap data sets is that locations are not where they are supposed to be. The safest way to check your data is to plot them... preferably R! After synthesizing >100 different projects from different data contributors for one project, we found ~20%(!) of submissions had a clear and obvious location errors (e.g. a camera station in the middle of the Atlantic).

Don't just take my word for it:

```{r, echo=F,  out.width="50%"}
knitr::include_graphics("images/exploration/project_your_locations.PNG")
```

(p.s. Mason is well worth a follow on Twitter)

Below we make use of the fantastic 'leaflet' package to produce interactive plots to help us check our station locations. `leaflet' has a tonne of different customizations and freely available, high resolution, base layers to choose from. 

##### Simple leaflet map

Note - Leaflet is best used using tidyverse 'pipe' notation  - `%>%`. It allows you to add successive operation in order. 

```{r map1, echo=F}
# call leaflet
m <- leaflet() %>%
  # add a nice base layer
  addProviderTiles(providers$Esri.WorldTopoMap, group="Base") %>%
  # add circles where each of your deployments is located
  addCircleMarkers(lng=dep$longitude, lat=dep$latitude) 
m

```

This looks correct to me - we do not have any stations in the Atlantic (phew)! But we can make this plot even more useful with a few customization options:

##### A better leaflet map

```{r map2, echo=F}


# First, set a single categorical variable of interest from station covariates for summary graphs. If you do not have an appropriate category use "project_id".
category <- "feature_type"


# First lets choose a category to colour
dep[,category] <- factor(dep[,category])
col.cat <- turbo(length(levels(dep[,category])))
# Add it to the dataframe
dep$colours <- col.cat[dep[,category]]



m <- leaflet() %>%
  # Add a satellite image layer
  addProviderTiles(providers$Esri.WorldImagery, group="Satellite") %>%  
  addProviderTiles(providers$Esri.WorldTopoMap, group="Base") %>%     
  addCircleMarkers(lng=dep$longitude, lat=dep$latitude,
                   # Colour the markers depending on the 'feature type'
                   color=dep$colours,
                   # Add a popup of the deployment code 
                   popup=paste(dep$placename, dep[,category])) %>%
  # Add a legend explaining what is going on
  addLegend("bottomleft", colors = col.cat,  labels = levels(dep[,category]),
    title = category,
    labFormat = labelFormat(prefix = "$"),
    opacity = 1
  ) %>%
  # add a layer control box to toggle between the layers
  addLayersControl(
    baseGroups = c("Satellite", "Base"),
    options = layersControlOptions(collapsed = FALSE)
  )
m

```

If you click on a point you will see it's corresponding `deployment_id code` - so you can find the problem data. You can also check your treatment categories using the key. If you zoom in, all the "online" categories should be on a linear feature, and the 'offline' stations should be >100m away from linear features.   

#### Distance between camera pairs
Sometime the corrdinates of a camera stations are accidently repeated in the deployment data, wthis is very hard to see on a map, as the points will overlay perfectly. The way we check this is to calculate the pairwise distance between all of the cameras in the project. This helps us in two ways:

- we can find "cryptic" duplications in coordinates
- this distance is often reported in manuscripts

#### Determine distances between all pairs of cameras
```{r, echo =F}


# create spatial file
camera_locs <- dep %>% 
  select(placename, latitude, longitude) %>% 
  unique() %>% # remove unique rows (in case of multiple deployment)
  st_as_sf(coords = c("longitude", "latitude"), crs = "+proj=longlat")

# distance matrix for all cameras
camera_dist <- st_distance(camera_locs) %>% 
                  as.dist() %>% 
                  usedist::dist_setNames(as.character(camera_locs$placename)) %>% 
                  as.matrix()

# convert to pairwise list
camera_dist_list <- t(combn(colnames(camera_dist), 2))
camera_dist_list <- data.frame(camera_dist_list, dist = camera_dist[camera_dist_list]) %>% 
                          arrange(dist) # sort descending

# Remove stations that have the same name
camera_dist_list <- camera_dist_list[!(camera_dist_list$X1==camera_dist_list$X2),]

# create a list of the closest camera for each station
camera_dist_list <- camera_dist_list %>% 
    group_by(X1) %>% 
    slice(which.min(dist))

```

Lets summarise the output:

```{r}
summary(camera_dist_list$dist)
```

So the largest distance between two cameras is `r paste0(round(max(camera_dist_list$dist),0), "m")`, the minimum is `r paste0(round(min(camera_dist_list$dist)), "m")` and on average it is `r paste0(round(mean(camera_dist_list$dist)), "m")`.

Looks good to me!


#### Do all images have a deployment associated with them?

Lets to a quick check to see if all of the placenames in the image data are represented in the deployment data. You would be surprised how often this is not the case!

```{r}
# check all check the placenames in images are represented in deployments 
table(unique(img$placename) %in% unique(dep$placename))
```

```{r}
# check all the placenames in deployments are represented in the images data
table(unique(dep$placename)  %in% unique(img$placename))
```

If you see any FALSE observations - then something is missing. Go back and check your raw data!

### Camera activity

Undoubtedly the most common issue we see with camera data is issues with camera activity, with nonsensical dates or start and end dates frequent. We use a dot and line plot to check if our cameras are active when we think they are. Here we make use of the 'plotly' package, a tool which allows you to produce interactive graphics.


Black dots denote start and end dates, lines denote periods where a camera is active. Each unique 'placename' gets its own row on the plot. 

```{r activity, echo=T}

# Call the plot
p <- plot_ly()

# We want a separate row for each 'placename' - so lets turn it into a factor
dep$placename <- as.factor(dep$placename)

# loop through each place name
for(i in seq_along(levels(dep$placename)))
  {
      #Subset the data to just that placename
      tmp <- dep[dep$placename==levels(dep$placename)[i],]
      # Order by date
      tmp <- tmp[order(tmp$start_date),]
      # Loop through each deployment at that placename
      for(j in 1:nrow(tmp))
      {
        # Add a line to 'p'
        p <- add_trace(p, 
                       #Use the start and end date as x coordinates
                       x = c(tmp$start_date[j], tmp$end_date[j]), 
                       #Use the counter for the y coordinates
                       y = c(i,i), 
                       # State the type of chart
                       type="scatter",
                       # make a line that also has points
                       mode = "lines+markers", 
                       # Add the deployment ID as hover text
                       hovertext=tmp$deployment_id[j], 
                       # Colour it all black
                       color=I("black"), 
                       # Supress the legend
                       showlegend = FALSE)
      }
      
  }

p

```

Can you see any issues? 

### Detection check
We now need to check if all of our labelled images fall within the deployment periods associated with them. To do this we use the plot shown above, but also add in the detection data over the top. This plot can get very messy, so we divide it into sections of ten deployments. Here I only show the first 10, but you should do this for all of your deployments!

**Step 1** check the black format

```{r}
table(img$is_blank)
```

```{r}
# Make a separate plot for each 20 stations For each 20 stations
# To do this make a plot dattaframe
tmp <- data.frame("deployment_id"=unique(dep$deployment_id), "plot_group"=ceiling(1:length(unique(dep$deployment_id))/20))

dep_tmp <- left_join(dep,tmp, by="deployment_id")

for(i in 1:max(dep_tmp$plot_group))
{  
  # Call the plot
  p <- plot_ly() 
  
  #Subset the data to just that placename
  tmp <- dep_tmp[dep_tmp$plot_group==i,]
  # Order by placename 
  tmp <- tmp[order(tmp$placename),]
  
 
 # Loop through each deployment at that placename
  for(j in 1:nrow(tmp))
    {
        #Subset the image data
        tmp_img <- img[img$deployment_id==tmp$deployment_id[j],]
        
        if(nrow(tmp_img)>0)
        {
         
          p <- add_trace(p, 
                       #Use the start and end date as x coordinates
                       x = c(tmp_img$timestamp), 
                       #Use the counter for the y coordinates
                       y = rep(j, nrow(tmp_img)), 
                       # State the type of chart
                       type="scatter",
                       # make a line that also has points
                       mode = "markers", 
                       # Add the deployment ID as hover text
                       hovertext=paste(tmp_img$genus,tmp_img$species), 
                       # Colour it all black
                       marker = list(color = "red"), 
                       # Supress the legend
                       showlegend = FALSE)
        }
        
       # Add a line to 'p'
        p <- add_trace(p, 
                       #Use the start and end date as x coordinates
                       x = c(tmp$start_date[j], tmp$end_date[j]), 
                       #Use the counter for the y coordinates
                       y = c(j,j), 
                       # State the type of chart
                       type="scatter",
                       # make a line that also has points
                       mode = "lines", 
                       # Add the deployment ID as hover text
                       hovertext=tmp$deployment_id[j], 
                       # Colour it all black
                       color=I("black"), 
                       # Supress the legend
                       showlegend = FALSE)
      }
  # Add custom y axis labels  
  p <- p %>%   layout(yaxis = list(

      ticktext = as.list(tmp$deployment_id), 

      tickvals = as.list(1:nrow(tmp)),

      tickmode = "array"))
  
  print(p)
      
  
} 


dep[dep$deployment_id=="ALG068_2017-11-10",]


```

**WHAT DO IF YOU HAVE AN ISSUE**

**CORRECT DEPLOYMENT DATES**

**CORRECT IMAGES DATES**

### Taxonomy check

Dealing with taxonomy in camera trap datasets can be a nightmare, particulalrly if your data labelling software does not give standardised lists of species (e.g. you are manulally sorting images into folders). 
Let us start with looking through the taxonomic classifications manually:

```{r}

taxonomy_headings <- c("class", "order", "family", "genus", "species", "common_name")

tmp<- img[,colnames(img)%in% taxonomy_headings]
tmp <- tmp[duplicated(tmp)==F,]
sp_list  <- tmp[order(tmp$class, tmp$order, tmp$family, tmp$genus, tmp$species),]

sp_list %>%
  kbl(row.names=F) %>%
  kable_styling(full_width = T) 
```

Thats a lot of species - are they all correct?

Maybe having common names would help too? But that is a lot of googling...

Lets interrogate the databases for a single species first.

```{r}
gnr_resolve("Lynx canadensis")
```

For each hit in different data bases, you get a row in a dataframe, and a confidence score in the identification. 

Lets try miss-spelling a common name:

```{r}
gnr_resolve("Lynx cramadensis")
```

Wow! So we can even recover incorrectly spelt latin names. Fantastic. 

Let us tun this code into a useful work flow! We will use a confidence threshold of 90% for out designations.


```{r}
# First add a column to the species list with genus and species pasted together
sp_list$sp <- paste(sp_list$genus, sp_list$species)

# Add a column which states if it is in the databases
sp_list$verified <- NA

# Check if it is in a bunch of different taxonomic databases
for(i in 1:nrow(sp_list))
{
  tmp <- gnr_resolve(sp_list$sp[i])
  
  if(nrow(tmp[tmp$score>0.9,])>0)
  {
    sp_list$verified[i] <- TRUE
  } else{sp_list$verified[i] <- FALSE}
}

sp_list

# Update the row names
row.names(sp_list) <- 1:nrow(sp_list)

```

Lets take another look at that list:

```{r}
sp_list %>%
  kbl(row.names=F) %>%
  kable_styling(full_width = T) 

```

Most of our data was verified - great! But what about those common names. Well there is a way we can get those too using `sci2comm()`"

```{r}
sci2comm("Lynx canadensis")
```

Lets run it through all of our data (as above):

```{r}
# First add a column to the species list with genus and species pasted together
sp_list$sp <- paste(sp_list$genus, sp_list$species)

# Check if it is in a bunch of different taxonomic databases
for(i in 1:nrow(sp_list))
{
  tmp <- sci2comm(sp_list$sp[i])
  
  if(length(tmp[[1]])>0)
  {
    sp_list$common_name[i] <- tmp[[1]]
  } 
}

```

Let's take another look: 

```{r}
sp_list %>%
  kbl(row.names=F) %>%
  kable_styling(full_width = T) 

```

Not bad - let's manually update the reamining species. It is definately far better than googling all of them!

```{r}
sp_list$common_name[sp_list$sp=="Perisoreus canadensis"] <- "Canada jay"
sp_list$common_name[sp_list$sp=="Colaptes auratus"] <- "northern flicker"
sp_list$common_name[sp_list$sp=="Cervus canadensis"] <- "elk"

```

Lets export this list right now, we will return to it later to add other traits which we may use in analysis. 


Then let's write our species list into it:

```{r}
write.csv(sp_list, paste0("data/raw_data/raw_species_list.csv")
```

Then lets update the `common_name` column in our `img` dataframe to reflect the common names.

We will do this using a left join, an operation which is invaluable when programming in R. It uses a specified "key" variable to merge two dataframes in this case we will use the 'sp' column. Check it out:

```{r}
# first remove the common_name column
img$common_name <- NULL

# add an sp column to the img dataframe - remember the genus and species columns are not pasted together yet
img$sp <- paste(img$genus, img$species)

# Next we do the 'left_join'
img <- left_join(img, sp_list[, c("sp", "common_name")], by="sp")

head(img)

```


**making corrections** DO A SECTION ON THIS???


## Diel activity check

Sometimes when setting up a camera trap, you can input the time incorrectly. This is actually very hard to detect unless you happen to be looking for it. The way we check is to plot the detections for each species out by the 24 hour clock. If were get detections of nocturnal species in the day, or vice versa, it suggest there may be a problem. 

*NOTE* this doesnt mean there is actually a problem, camera traps have revealed that many animals are active when we thought they were not!

*NOTE 2* Researchers are increasingly using this information to determine a species "availability" for detection! More on that in the [density chapter](#density). 

For any species detected more than 10 times, we will plot when they were detected:

```{r, echo=F, warning=F, message=F}
img$hours <- hour(img$timestamp) + minute(img$timestamp)/60 + second(img$timestamp)/(60*60)
# Susbet to more than 100 captures


tmp <- img %>% group_by(common_name) %>% summarize(count=n())

tmp <-tmp[tmp$count>10,]

yform <- list(categoryorder = "array",
              categoryarray = tmp$common_name)
tmp2 <- img[img$common_name %in% yform$categoryarray,]


fig <- plot_ly(x = tmp2$hours, y = tmp2$common_name,type="scatter",
               height=1000, text=tmp2$deployment_id, hoverinfo='text',
               mode   = 'markers',
               marker = list(size = 5,
                             color = 'rgba(50, 100, 255, .2)',
                             line = list(color = 'rgba(0, 0, 0, 0)',
                                         width = 0))) %>% 
              layout(yaxis = yform)
fig

# Remove the column
img$hours <- NULL
```


```{r, eval=F, include=F, echo=F}


### Other labelling attributes
As you progress with image labeling, it is important to check that the additional information you are collecting is consistent across species. In our case, we often try to record the sex, age class and behavior of detected wildlife (where identifiable).

Of the images classified as containing animals, the proportion of photographs assigned to the following categories are as follows:

**Sex**
```{r sex, echo=F, include=F, eval=F}
col.name <- "Sex"

plot<-FALSE
if(length(colnames(dat)[colnames(dat)==col.name]>0))
   {
      tmp <- table(dat[,col.name][dat$Blank==FALSE], as.character(dat$Species[dat$Blank==FALSE]))
      tmp <- as.data.frame.matrix(tmp)
      
      dat[,col.name]<- factor(dat[,col.name])
      cols <- turbo( length(levels(dat[,col.name])))
      # Name catagories with no data N\A for NOT ASSESSED
      row.names(tmp)[row.names(tmp)==""] <- "N/A"
      # make it the last level
      tmp <- tmp[c(2:nrow(tmp),1),]
      
      data_percentage <- apply(tmp, 2, function(x){x*100/sum(x,na.rm=T)})
      plot<-TRUE
  }

#```


#```{r sex plot, echo=F, fig.height=sp.height, eval=F}
if(plot==TRUE)
{
layout(matrix(c(1,1,1,2), 1, 4, byrow = TRUE))
par(mar=c(5,10,1,1))
barplot(data_percentage , border="white",col= cols, ylab="", las=1, xlab="% of observations", cex.names=0.7, horiz=2)
par(mar=c(0,0,4,0))
plot.new()
legend("topleft", legend=row.names(tmp), fill=cols, xpd=TRUE, cex=1.1 )
} else { print('Not included') }
```

**Age**

```{r age, echo=F, include=F, eval=F}
col.name <- "Age"

plot<-FALSE
if(length(colnames(dat)[colnames(dat)==col.name]>0))
   {
      tmp <- table(dat[,col.name][dat$Blank==FALSE], as.character(dat$Species[dat$Blank==FALSE]))
      tmp <- as.data.frame.matrix(tmp)
      
      dat[,col.name]<- factor(dat[,col.name])
      cols <- turbo( length(levels(dat[,col.name])))
      # Name catagories with no data N\A for NOT ASSESSED
      row.names(tmp)[row.names(tmp)==""] <- "N/A"
      # make it the last level
      tmp <- tmp[c(2:nrow(tmp),1),]
      
      data_percentage <- apply(tmp, 2, function(x){x*100/sum(x,na.rm=T)})
      plot<-TRUE
  }

##```


#```{r age plot, echo=F, fig.height=sp.height, eval=F}
if(plot==TRUE)
{
layout(matrix(c(1,1,1,2), 1, 4, byrow = TRUE))
par(mar=c(5,10,1,1))
barplot(data_percentage , border="white",col= cols, ylab="", las=1, xlab="% of observations", cex.names=0.7, horiz=2)
par(mar=c(0,0,4,0))
plot.new()
legend("topleft", legend=row.names(tmp), fill=cols, xpd=TRUE, cex=1.1 )
} else { print('Not included') }

#```

**Behaviour**

#```{r Behaviour, echo=F, include=F, eval=F}
col.name <- "Behaviour"
plot<-FALSE
if(length(colnames(dat)[colnames(dat)==col.name]>0))
   {
      tmp <- table(dat[,col.name][dat$Blank==FALSE], as.character(dat$Species[dat$Blank==FALSE]))
      tmp <- as.data.frame.matrix(tmp)
      
      dat[,col.name]<- factor(dat[,col.name])
      cols <- turbo( length(levels(dat[,col.name])))
      # Name catagories with no data N\A for NOT ASSESSED
      row.names(tmp)[row.names(tmp)==""] <- "N/A"
      # make it the last level
      tmp <- tmp[c(2:nrow(tmp),1),]
      
      data_percentage <- apply(tmp, 2, function(x){x*100/sum(x,na.rm=T)})
      plot<-TRUE
  }
#```


#```{r behaviour plot, echo=F, fig.height=sp.height, eval=F}
if(plot==TRUE)
{
layout(matrix(c(1,1,1,2), 1, 4, byrow = TRUE))
par(mar=c(5,10,1,1))
barplot(data_percentage , border="white",col= cols, ylab="", las=1, xlab="% of observations", cex.names=0.7, horiz=2)
par(mar=c(0,0,4,0))
plot.new()
legend("topleft", legend=row.names(tmp), fill=cols, xpd=TRUE, cex=1.1 )
} else { print('Not included') }
#```



```


# Analysis data creation

## Common analysis data formats

Although the types of analysis you can perform on camera trap data vary markedly, they often depend on three key dataframe structures. We introduce these structures here, then show you how to apply them in subsequent chapters. 

## Independent detections
The independent detections dataframe is the work horse of all camera trap analyses, it is from this that you build the rest of your data frames. The threshold we use for determining what is an "independent detection" is typically 30 minutes... because camera trappers are creatures of habit! If you want to dig a little deeper it to the why, there is a nice summary in [Rahel Sollmans "A gentle introduction to camera‐trap data analysis"](https://onlinelibrary.wiley.com/doi/abs/10.1111/aje.12557):

*Researchers have used different thresholds, typically 30 min (e.g., O'Brien, Kinnaird, & Wibisono, 2003) to an hour (Bahaa‐el‐din et al., 2016); some researchers have argued that multiple pictures within the same day may not represent independent detections (Royle, Nichols, Karanth, & Gopalaswamy, 2009). In most cases, this threshold is determined subjectively, based on the best available knowledge of the species under study. But it can also be determined based on the temporal autocorrelation (Kays & Parsons, 2014) or analysis of time intervals (Yasuda, 2004) of subsequent pictures.*

The default setting in our data processing script is 30 minutes, although it can be changed! 


**EXAMPLE IMAGE**

## Effort look-up
Image data without effort data is worthless! There are lots of instances where you need to know which stations were operating on a given day. Some people like to store this information in a siteXdate matrix, but they are actually not that easy to data wrangle with. A long data frame with a site and date column is the most flexible (and keeps the dates in their native POSIX formats).

**EXAMPLE IMAGE**

## Counts/detections by time interval
We saved the most useful data format until last! A site, time interval, effort, and species  count dataframe integrates the independent data and daily lookup described above. You ca use it to create detection rates, occupancy data frames and much more!

We export yearly, monthly, weekly and daily data frames from our single site exploration script (which should cover you for much of what you want to do).

We include two different types of response terms:

  - Observations = the number of independent detections per time interval
  - Counts = sum of the independent minimum group sizes per time interval


**EXAMPLE IMAGE**

## Our data

First, lets create the folder to store our data!

```{r}
dir.create("data/processed_data")
```

This section will follow the following steps:

1) Filter to our target species

2) Create a camera activity look-up 

3) Determine our "independent detections"

4) Create our analysis data frames

## 1) Filter to target species


```{r}
# Remove observations without animals detected, whre we dont know the species, and non-mammals

img_sub <- img[img$is_blank==0 &              
                 is.na(img$species)==FALSE &  
                 img$class=="Mammalia" &      
                 img$species!="sapiens",]            


img_sub <- img %>% filter(is_blank==0 &     # Remove the blanks
                           is.na(img$species)==FALSE & # Remove classifications which dont have species 
                           class=="Mammalia", # Subset to mammals
                           species!="sapiens") # Subset to anything that isn't human



```

This has resulted in the removal of `r round((1-nrow(img_sub)/nrow(img))*100,1)`% of the observations. 

```{r}
table(img_sub$common_name)
```

## 2) Create a camera activity lookup

One step I like to perform is to create a list of every single day when a given camera is active. It is very useful to filter out detections which occured outside of camera activity periods (step 3) and create our analysis dataframes (Step 4)

```{r}
# Remove all observations with occur outside of camera activity schedules
# We need to know how many detections there are in each month -> create a row lookup
# This is just a list of ever day a camera was active.

tmp <- dep[is.na(dep$end_date)==F,]
daily_lookup <- list()
for(i in 1:nrow(tmp))
{
  if(ymd(tmp$start_date[i])!=ymd(tmp$end_date[i]))
  {
    daily_lookup[[i]] <- data.frame("date"=seq(ymd(tmp$start_date[i]), ymd(tmp$end_date[i]), by="days"), "placename"=tmp$placename[i])
  }
}
row_lookup <- bind_rows(daily_lookup)

# Remove duplicates (just in case)
row_lookup <- row_lookup[duplicated(row_lookup)==F,]

```


## 3)Determine 'independent' camera detections
We rarely analyse raw camera data, rather we filter out multiple detections of the same individual within a given event. This is called creating and "independent detections" dataframe. 

The use of an independence threshold of 30 minutes is often used - and I will not go into the reasons for that now. However, it is wise to think about what you are analyzing and whether such a threshold is appropriate. For example, if your organism of interest is very abundant, for examples human hikers on a busy trail, then using a 30 minute threhold may meen that multiple independent groups of hikers are rolled into a single, huge, "event".

```{r}
# Set the "independence" interval in minutes
independent <- 30
```

I will now break down the algorithm into subsetions to make it clear what is occuring:

i) Order the dataframe by deployment code and species

```{r echo=T, eval=T, message = F, warning = F}

img_tmp <- img_sub %>%
              arrange(project_id,deployment_id) %>%
              group_by(deployment_id, sp) %>%
              mutate(duration = int_length(timestamp %--% lag(timestamp)))

```

ii) Determine independence

If subsequent detections occur outside of the independence threshold, assign it a unique ID code.

```{r echo=T, eval=T, message = F, warning = F}
library(stringr)
# Give a random value to all cells
img_tmp$event_id <- 9999

# Create a counter
counter <- 1

# Make a unique code that has one more zero than rows in your dataframe  
num_code <- as.numeric(paste0(nrow(img_sub),0))

for (i in 2:nrow(img_tmp)) {
  img_tmp$event_id[i-1]  <- paste0("E", str_pad(counter, nchar(num_code), pad = "0"))
  
  if(is.na(img_tmp$duration[i]) | abs(img_tmp$duration[i]) > (independent * 60))
    {
      counter <- counter + 1
    }
}

# Update the information for the last row - the loop above always updates the previous row... leaving the last row unchanbged
   
 # group ID  for the last row
 if(img_tmp$duration[nrow(img_tmp)] < (independent * 60)|
    is.na(img_tmp$duration[nrow(img_tmp)])){
   img_tmp$event_id[nrow(img_tmp)] <- img_tmp$event_id[nrow(img_tmp)-1]
 } else{
   counter <- counter + 1
   img_tmp$event_id[nrow(img_tmp)] <- paste0("E", str_pad(counter, nchar(num_code), pad = "0"))
 }

# remove the duration column
img_tmp$duration <- NULL
 
```

We could stop there, however there is other information we might light to extract about each individual event:

a) the maximum number objects detected in an event
b) how long the event lasts
c) how many images are in each event


```{r echo=T, eval=T, message = F, warning = F}

  # find out the last and the first of the time in the group
  top <- img_tmp %>% group_by(event_id) %>% top_n(1,timestamp) %>% select(event_id, timestamp)
  bot <- img_tmp %>% group_by(event_id) %>% top_n(-1,timestamp) %>% select(event_id, timestamp)
  names(bot)[2] <- c("timestamp_end")
  
  img_num <- img_tmp %>% group_by(event_id) %>% summarise(event_observations=n()) # number of images in the event
  event_grp <- img_tmp %>% group_by(event_id) %>% summarise(event_groupsize=max(number_of_objects))

  # caculate the duration and add the other elements
  diff <-  top %>% left_join(bot, by="event_id") %>%
      mutate(event_duration=abs(int_length(timestamp %--% timestamp_end))) %>%
      left_join(event_grp, by="event_id")%>%
      left_join(img_num, by="event_id")

  # Remove columsn you dont need
  diff$timestamp   <-NULL
  diff$timestamp_end <-NULL

    # Merge the img_tmp with the event data
  img_tmp <-  img_tmp %>%
   left_join(diff,by="event_id")

```

Finally lets subset to the first row of each event to create our independent dataframe!

```{r}
ind_dat <- img_tmp[duplicated(img_tmp$event_id)==F,]
```


Next we remove any detections which occur outside of our known camera activity periods:

```{r}
# Make a  iunique code for ever day and deployment where cameras were functioning
tmp <- paste(row_lookup$date, row_lookup$placename)

#Subset ind_dat to data that matches the unique codes
ind_dat <- ind_dat[paste(substr(ind_dat$timestamp,1,10), ind_dat$placename) %in% tmp, ]

```

As a final step, we make the species column a 'factor' - this makes all the data frame building operations much simpler:

```{r}
ind_dat$sp <- as.factor(ind_dat$sp)
```


## 4) Creating analysis dataframes
Finally, this script outputs 11 useful data frames for future data analysis:

1. A data frame of "independent detections" at the `r independent` minute threshold you specified at the start: 

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Independent.csv")`"

```{r}
write.csv(ind_dat, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_detectons.csv"), row.names = F)
```
 
2. The "daily_lookup" which is a dataframe of all days a given camera station was active. Some people use an lookup matrix for this step, but we find the long format is much easier to use in downstream analysis. 
  - "`r paste0("data/processed_data/",ind_dat$project_.ID_id[1], "_daily_deport_lookup.csv")`"  

```{r}
write.csv(row_lookup, paste0("data/processed_data/",ind_dat$project_id[1], "_daily_lookup.csv"), row.names = F)
```

3. Unique camera locations list:

When we start to build the covariates for data analysis, it is very useful to have a list of your final project's camera locations. We create this below in a simplified form. You can include any columns which will be use for data analysis, and export it. 

```{r}
tmp <- dep[, c("project_id", "placename", "longitude", "latitude", "feature_type")]
tmp<- tmp[duplicated(tmp)==F,]
write.csv(tmp, paste0("data/processed_data/",ind_dat$project_id[1], "_camera_locations.csv"), row.names = F)
```

4. Final species list

```{r}
tmp <- sp_list[sp_list$sp %in% ind_dat$sp,]

write.csv(tmp, paste0("data/processed_data/",ind_dat$project_id[1], "_species_list.csv"), row.names = F)
```



5 & 6: A 'site x species' matrix of the number of independent detections and species counts across the full study period:

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Independent_total_observations.csv")`"

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Independent_total_counts.csv")`"


```{r, echo=F, message=F, warning=F}
# Total counts
  # Station / Month / deport / Species      
  tmp <- row_lookup
  
  # Calculate the number of days at each site  
  total_obs <- tmp %>% 
      group_by(placename) %>%
      summarise(days = n())
  
  # Convert to a data frame
  total_obs <- as.data.frame(total_obs)
  
  # Add columns for each species  
  total_obs[, levels(ind_dat$sp)] <- NA
  # Duplicate for counts
  total_count <- total_obs
  # Test counter
  i <-1
  # For each station, count the number of individuals/observations
  for(i in 1:nrow(total_obs))
    {
      tmp <- ind_dat[ind_dat$placename==total_obs$placename[i],]
      
      tmp_stats <- tmp %>%  group_by(sp, .drop=F) %>% summarise(obs=n(), count=sum(number_of_objects))
      
      total_obs[i,as.character(tmp_obs$sp)] <- tmp_stats$obs
      total_count[i,as.character(tmp_count$sp)] <- tmp_stats$count
    }

  
# Save them
    
write.csv(total_obs, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_total_observations.csv"), row.names = F) 

write.csv(total_count, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_total_counts.csv"), row.names = F) 

```


7 & 8: A 'site_month x species' matrix of the number of independent detections and species counts across for each month in the study period:

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Monthly_total_observations.csv")`"

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Monthly_total_counts.csv")`"



```{r, echo=F, message=F, warning=F}

# Monthly counts
  # Station / Month / days / Covariates / Species      
  tmp <- row_lookup
  # Simplify the date to monthly
  tmp$date <- substr(tmp$date,1,7)
  
  # Calculate the number of days in each month  
  mon_obs <- tmp %>% 
      group_by(placename,date ) %>%
      summarise(days = n())
  # Convert to a data frame
  mon_obs <- as.data.frame(mon_obs)
    
  mon_obs[, levels(ind_dat$sp)] <- NA
  mon_count <- mon_obs
  # For each month, count the number of individuals/observations
  for(i in 1:nrow(mon_obs))
    {
      tmp <- ind_dat[ind_dat$placename==mon_obs$placename[i] & substr(ind_dat$timestamp,1,7)== mon_obs$date[i],]
      
      tmp_stats <- tmp %>%  group_by(sp, .drop=F) %>% summarise(obs=n(), count=sum(number_of_objects))
      
      mon_obs[i,as.character(tmp_obs$sp)] <- tmp_stats$obs
      mon_count[i,as.character(tmp_count$sp)] <- tmp_stats$count
      
    }

  
write.csv(mon_obs, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_monthly_observations.csv"), row.names = F) 

write.csv(mon_count, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_monthly_counts.csv"), row.names = F) 

```



9 & 10: A 'site_week x species' matrix of the number of independent detections and species counts across for each week in the study period:

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Weekly_total_observations.csv")`"

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Weekly_total_counts.csv")`"
  
  
  

```{r, echo=F, message=F, warning=F}
# Weekly format
  # Station / Month / days / Covariates / Species      
  tmp <- row_lookup
  # Simplify the date to year-week
  tmp$date <- strftime(tmp$date, format = "%Y-W%U")
  # The way this is coded is the counter W01 starts at the first sunday of the year, everything before that is W00. Weeks do not roll accross years.
  
  # Calculate the number of days in each week  
  week_obs <- tmp %>% 
      group_by(placename,date ) %>%
      summarise(days = n())
  
  # Convert to a data frame
  week_obs <- as.data.frame(week_obs)
  
  # Add species columns  
  week_obs[, levels(ind_dat$sp)] <- NA
  
  # Duplicate for counts
  week_count <- week_obs
  
  # For each week, count the number of individuals/observations
  for(i in 1:nrow(week_obs))
    {
      tmp <- ind_dat[ind_dat$placename==week_obs$placename[i] & strftime(ind_dat$timestamp, format = "%Y-W%U")== week_obs$date[i],]
      
      tmp_stats <- tmp %>%  group_by(sp, .drop=F) %>% summarise(obs=n(), count=sum(number_of_objects))
      
      week_obs[i,as.character(tmp_obs$sp)] <- tmp_stats$obs
      week_count[i,as.character(tmp_count$sp)] <- tmp_stats$count
      
    }

write.csv(week_obs, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_weekly_observations.csv"), row.names = F) 

write.csv(week_count, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_weekly_counts.csv"), row.names = F) 

```  
  
11 & 12: A 'site_day x species' matrix of the number of independent detections and species counts across for each day a station was active in the study period:

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Daily_total_observations.csv")`"

  - "`r  paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_Daily_total_counts.csv")`"
  
```{r, echo=F, message=F, warning=F}
# Daily format
  # Station / Month / days / Covariates / Species      
  tmp <- row_lookup
  tmp$days <- 1
  # Add species columns  
  tmp[, levels(ind_dat$sp)] <- NA
  
  day_obs <- tmp
  day_count <- tmp
# For each week, count the number of individuals/observations
  for(i in 1:nrow(day_obs))
    {
      tmp <- ind_dat[ind_dat$placename==day_obs$placename[i] & strftime(ind_dat$timestamp, format = "%Y-%m-%d")== day_obs$date[i],]
      
      tmp_stats <- tmp %>%  group_by(sp, .drop=F) %>% summarise(obs=n(), count=sum(number_of_objects))
      
      day_obs[i,as.character(tmp_obs$sp)] <- tmp_stats$obs
      day_count[i,as.character(tmp_count$sp)] <- tmp_stats$count
        
      
    }
write.csv(day_obs, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_daily_observations.csv"), row.names = F) 

write.csv(day_count, paste0("data/processed_data/",ind_dat$project_id[1], "_",independent ,"min_independent_daily_counts.csv"), row.names = F) 

```


**Final data check**

Finally, as a last check that our code is creating robust analysis data frames, we check if the observations/counts are the same across each temporal scale (total/monthly/weekly/daily). Check this using the following tables. 

**Observations**
```{r, echo=F}

tmp <- cbind(data.frame("Time"=c("Total", "Monthly", "Weekly", "Daily")),
rbind(colSums(total_obs[,2:ncol(total_obs)]),
colSums(mon_obs[,3:ncol(mon_obs)]),
colSums(week_obs[,3:ncol(week_obs)]),
colSums(day_obs[,3:ncol(day_obs)])  ))

tmp %>%
  kbl() %>%
  kable_styling(full_width = T) %>%
  column_spec(1, bold = T, border_right = T)

```

** Counts **
```{r, echo=F}
tmp <- cbind(data.frame("Time"=c("Total", "Monthly", "Weekly", "Daily")),
rbind(colSums(total_count[,2:ncol(total_count)]),
colSums(mon_count[,3:ncol(mon_count)]),
colSums(week_count[,3:ncol(week_count)]),
colSums(day_count[,3:ncol(day_count)])  ))

tmp %>%
  kbl() %>%
  kable_styling(full_width = T) %>%
  column_spec(1, bold = T, border_right = T)

```









