# Data Exploration

**CHRIS USE THE WILDLIFE INSIGHTS ONE -> TAXONOMY RESOLVER TOOOOOOOO**
The most important part of analyzing camera trap data is exploring and checking your data. 

Dynamic data exploration is key - check your data as you go. If you leave it to the end of your data collection and processing, the opportunity to correct mistakes early will be lost! 

## Standardised exploration script

In the Wildlife Coexistence lab we use a standardized R script to check the data generated by camera trap projects. 

The most up-to-date script for exploring a single site is available on our [GitHub page](https://github.com/WildCoLab/SingleSiteExploration).

Below we run through the key plots and outputs this script generates.

The first step is to read in your data:

```{r}
# Load your data 
pro <- read.csv("data/raw_data/example_data/proj.csv", header=T)
img <- read.csv("data/raw_data/example_data/img.csv", header=T)
dep <- read.csv("data/raw_data/example_data/dep.csv", header=T)
cam <- read.csv("data/raw_data/example_data/cam.csv", header=T)

```

Next, load in the packages we will use in this chapter:

```{r, echo=T, eval=F}
#Load Packages
list.of.packages <- c("leaflet", "dplyr", "viridis", "kriging", "corrplot", "lubridate", "kableExtra", "tidyr", "taxise")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages, require, character.only = TRUE)

```


```{r, include=F}
#Load Packages
list.of.packages <- c("leaflet", "dplyr", "viridis", "kriging", "corrplot", "lubridate", "kableExtra", "tidyr")
# Check you have them and load them
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages, require, character.only = TRUE)

```


## Format your dates

Dealing with date objects in R can be intimidating, but once you get the hang of it, it is incredibly powerful. The process has been made far easier with the 'lubridate' package. 

The first dates we need to convert are those in the `dep` datasheet - the start and end times of each deployment. The way 'lubridate' works is you specify the order of the days, months years, hours, minutes and seconds with the codes d,m,y,h,m, and s respectively. 

### Lubridate examples 
Try importing the 25th of December in a couple of formats:

```{r}
#library(lubridate)
# day-month-year
dmy("24-12-2022")

# year-month-day
ymd("2022-12-24")
```

The output should be identical. Note `lubridate` defaults to UTC - unless otherwise specified. 

Now the real power of lubridate lies in the fact that you can handle multiple different date formats in one column using the  `parse_date_time()` function. This sometimes happens - I usually blame excel - but you could be merging two datasets formatted in different ways too. Lets try it:

```{r}
x <- c("24-12-2022", "2022-12-24", "12-24-2022")
parse_date_time(x, c("ymd", "dmy", "mdy"))
```

They should give all the same output!

Next, lets calculate the amount of time which has elapsed between two dates. A fundamental operation in the managment of camera data. To do this we first create an interval object `interval(date1, date2)`, then ask to return the object in days `/ddays(1)`. Lets try it:

```{r, eval=F}
date1 <- ymd("2021-10-13")
date2 <- ymd("2021-12-11")

interval(date1, date2)/ddays(1)
```

How many days elapsed between those two dates?

### Deployment dates

Lets get back to the camera data. Which lubridate format should we use for `r dep$start_date[1]`?

`ymd()` should do the job.

```{r}
# start dates
dep$start_date <- ymd(dep$start_date)

# end dates
dep$end_date   <- ymd(dep$end_date)
```

Now lets make a new colum in the deployment data called `days`, and calcuklate the interval for all the deployments:

```{r}
dep$days <- interval(dep$start_date, dep$end_date)/ddays(1)
```

What are the range of dates the deployments were active for? Things to look out for are 0's, NA's and negative numbers. 

```{r, eval=F}
summary(dep$days)
```

**What does a values of zero mean?** Lets look at it:

```{r}
dep[dep$days==0,]
```

They are deployments which started and ended on the same day -> they must have failed. 

**what does an NA mean?**
```{r}
dep[is.na(dep$days)==T,]
```

This deployment does not even have an end date, it must have been stolen!

**What would a negative number mean?** 

Someone probably got the start and end date the wrong way round. It happens! Go back and check your datasheets. 

### Image dates
We next need to convert the `timestamp` column in the image dataframe. What `lubridate` format is required for a a date which looks like `r img$timestamp[1]`?

```{r}
ymd_hms("2015-11-21 03:03:44")
```

Lets apply this to our image dataset:

```{r}
img$timestamp <- ymd_hms(img$timestamp)
```

If you want to learn more about the amazing functionality of the 'lubridate' package - check out the pdf [Lubridate Cheatsheet](https://rawgit.com/rstudio/cheatsheets/main/lubridate.pdf)


```{r non-adjustable options, echo=F, include=F}

# Count the number of camera ststions
n.stat <- length(unique(dep$deployment_id))



# Generate colours to display the catagory levels - R needs them as a factor
#sta[,category] <- factor(sta[,category])
#col.cat <- turbo(length(levels(sta[,category])))
#dep$Cols <- col.cat[sta[,category]]

# # How big should the figures be
# dep.height <- 8
# if(length(unique(dep$deployment_id))>80)
#    {
#      dep.height <- length(unique(dep$deployment_id))/10
#    }
# 
# sp.height <- 7
# if(length(unique(dat$Species))>20)
#    {
#      sp.height <- 7+(length(unique(dat$Species))/8)
#    }


```

### Basic summaries

Now that our camera trap data are loaded into R, we can very quickly find out summary information about the dataset:

```{r}
# Number of unique locations - we assign this as a object as we use it elsewhere
 n.stat

# Number of records
nrow(img)

# Start and end date of the project
min(dep$start_date)
max(dep$end_date)

# Number of Blanks [NOT YET DONE]
#`r nrow(dat[dat$Blank==FALSE,])` are classified as blanks (`r round((nrow(dat[dat$Blank==TRUE,])/nrow(dat))*100,1)`% of the total data set). Of the detections which have been identified, there are `r length(levels(factor(dat$Species)))` different categories. 


```

### Quick error checks

There are some error checks which are so common that we want to get them out of the way right off the bat. If the dataset fails the following tests, then the subsequent scripts will definitely not run:


#### Camera locations

A common mistake in camera trap data sets is that locations are not where they are supposed to be. The safest way to check your data is to plot them... preferably R! After synthesizing >100 different projects from different data contributors for one project, we found ~20%(!) of submissions had a clear and obvious location errors (e.g. a camera station in the middle of the Atlantic).

Don't just take my word for it:

```{r, echo=F,  out.width="50%"}
knitr::include_graphics("images/exploration/project_your_locations.PNG")
```

(p.s. Mason is well worth a follow on Twitter)

Below we make use of the fantastic 'leaflet' package to produce interactive plots to help us check our station locations. `leaflet' has a tonne of different customizations and freely available, high resolution, base layers to choose from. 

##### Simple leaflet map

Note - Leaflet is best used using tidyverse 'pipe' notation  - `%>%`. It allows you to add successive operation in order. 

```{r map1, echo=F}
# call leaflet
m <- leaflet() %>%
  # add a nice base layer
  addProviderTiles(providers$Esri.WorldTopoMap, group="Base") %>%
  # add circles where each of your deployments is located
  addCircleMarkers(lng=dep$longitude, lat=dep$latitude) 
m

```

This looks correct to me - we do not have any stations in the Atlantic (phew)! But we can make this plot even more useful with a few customization options:

##### A better leaflet map

```{r map2, echo=F}


# First, set a single categorical variable of interest from station covariates for summary graphs. If you do not have an appropriate category use "project_id".
category <- "feature_type"


# First lets choose a category to colour
dep[,category] <- factor(dep[,category])
col.cat <- turbo(length(levels(dep[,category])))
# Add it to the dataframe
dep$colours <- col.cat[dep[,category]]



m <- leaflet() %>%
  # Add a satellite image layer
  addProviderTiles(providers$Esri.WorldImagery, group="Satellite") %>%  
  addProviderTiles(providers$Esri.WorldTopoMap, group="Base") %>%     
  addCircleMarkers(lng=dep$longitude, lat=dep$latitude,
                   # Colour the markers depending on the 'feature type'
                   color=dep$Cols,
                   # Add a popup of the deployment code 
                   popup=paste(dep$placename, dep[,category])) %>%
  # Add a legend explaining what is going on
  addLegend("bottomleft", colors = col.cat,  labels = levels(dep[,category]),
    title = category,
    labFormat = labelFormat(prefix = "$"),
    opacity = 1
  ) %>%
  # add a layer control box to toggle between the layers
  addLayersControl(
    baseGroups = c("Satellite", "Base"),
    options = layersControlOptions(collapsed = FALSE)
  )
m

```

If you click on a point you will see it's corresponding `deployment_id code` - so you can find the problem data. You can also check your treatment categories using the key. If you zoom in, all the "online" categories should be on a linear feature, and the 'offline' stations should be >100m away from linear features.   

#### Do all images have a deployment associated with them?

Lets to a quick check to see if all of the placenames in the image data are represented in the deployment data. You would be surprised how often this is not the case!

```{r}
# check all check the placenames in images are represented in deployments 
table(unique(img$placename) %in% unique(dep$placename))
```

```{r}
# check all the placenames in deployments are represented in the images data
table(unique(dep$placename)  %in% unique(img$placename))
```

If you see any FALSE observations - then something is missing. Go back and check your raw data!

### Camera activity

Undoubtedly the most common issue we see with camera data is issues with camera activity, with nonsensical dates or start and end dates frequent. We use a dot and line plot to check if our cameras are active when we think they are. Here we make use of the 'plotly' package, a tool which allows you to produce interactive graphics.


Black dots denote start and end dates, lines denote periods where a camera is active. Each unique 'placename' gets its own row on the plot. 

```{r activity, echo=T}

# Call the plot
p <- plot_ly()

# We want a separate row for each 'placename' - so lets turn it into a factor
dep$placename <- as.factor(dep$placename)

# loop through each place name
for(i in seq_along(levels(dep$placename)))
  {
      #Subset the data to just that placename
      tmp <- dep[dep$placename==levels(dep$placename)[i],]
      # Order by date
      tmp <- tmp[order(tmp$start_date),]
      # Loop through each deployment at that placename
      for(j in 1:nrow(tmp))
      {
        # Add a line to 'p'
        p <- add_trace(p, 
                       #Use the start and end date as x coordinates
                       x = c(tmp$start_date[j], tmp$end_date[j]), 
                       #Use the counter for the y coordinates
                       y = c(i,i), 
                       # State the type of chart
                       type="scatter",
                       # make a line that also has points
                       mode = "lines+markers", 
                       # Add the deployment ID as hover text
                       hovertext=tmp$deployment_id[j], 
                       # Colour it all black
                       color=I("black"), 
                       # Supress the legend
                       showlegend = FALSE)
      }
      
  }

p

```

Can you see any issues? 

### Detection check
We now need to check if all of our labelled images fall within the deployment periods associated with them. To do this we use the plot shown above, but also add in the detection data over the top. This plot can get very messy, so we divide it into sections of ten deployments. Here I only show the first 10, but you should do this for all of your deployments!

**Step 1** check the black format

```{r}
table(img$is_blank)
```

```{r}
# Make a separate plot for each 20 stations For each 20 stations
# To do this make a plot dattaframe
tmp <- data.frame("deployment_id"=unique(dep$deployment_id), "plot_group"=ceiling(1:length(unique(dep$deployment_id))/20))

dep_tmp <- left_join(dep,tmp, by="deployment_id")

for(i in 1:max(dep_tmp$plot_group))
{  
  # Call the plot
  p <- plot_ly() 
  
  #Subset the data to just that placename
  tmp <- dep_tmp[dep_tmp$plot_group==i,]
  # Order by placename 
  tmp <- tmp[order(tmp$placename),]
  
 
 # Loop through each deployment at that placename
  for(j in 1:nrow(tmp))
    {
        #Subset the image data
        tmp_img <- img[img$deployment_id==tmp$deployment_id[j],]
        
        if(nrow(tmp_img)>0)
        {
         
          p <- add_trace(p, 
                       #Use the start and end date as x coordinates
                       x = c(tmp_img$timestamp), 
                       #Use the counter for the y coordinates
                       y = rep(j, nrow(tmp_img)), 
                       # State the type of chart
                       type="scatter",
                       # make a line that also has points
                       mode = "markers", 
                       # Add the deployment ID as hover text
                       hovertext=paste(tmp_img$genus,tmp_img$species), 
                       # Colour it all black
                       marker = list(color = "red"), 
                       # Supress the legend
                       showlegend = FALSE)
        }
        
       # Add a line to 'p'
        p <- add_trace(p, 
                       #Use the start and end date as x coordinates
                       x = c(tmp$start_date[j], tmp$end_date[j]), 
                       #Use the counter for the y coordinates
                       y = c(j,j), 
                       # State the type of chart
                       type="scatter",
                       # make a line that also has points
                       mode = "lines", 
                       # Add the deployment ID as hover text
                       hovertext=tmp$deployment_id[j], 
                       # Colour it all black
                       color=I("black"), 
                       # Supress the legend
                       showlegend = FALSE)
      }
  # Add custom y axis labels  
  p <- p %>%   layout(yaxis = list(

      ticktext = as.list(tmp$deployment_id), 

      tickvals = as.list(1:nrow(tmp)),

      tickmode = "array"))
  
  print(p)
      
  
} 


dep[dep$deployment_id=="ALG068_2017-11-10",]


```

**WHAT DO IF YOU HAVE AN ISSUE**

**CORRECT DEPLOYMENT DATES**

**CORRECT IMAGES DATES**

### Taxonomy check

Dealing with taxonomy in camera trap datasets can be a nightmare, particulalrly if your data labelling software does not give standardised lists of species (e.g. you are manulally sorting images into folders). 
Let us start with looking through the taxonomic classifications manually:

```{r}

taxonomy_headings <- c("class", "order", "family", "genus", "species", "common_name")

tmp<- img[,colnames(img)%in% taxonomy_headings]
tmp <- tmp[duplicated(tmp)==F,]
sp_list  <- tmp[order(tmp$class, tmp$order, tmp$family, tmp$genus, tmp$species),]

sp_list %>%
  kbl(row.names=F) %>%
  kable_styling(full_width = T) 
```

Thats a lot of species - are they all correct?

Maybe having common names would help too? But that is a lot of googling...

Lets interrogate the databases for a single species first.

```{r}
gnr_resolve("Lynx canadensis")
```

For each hit in different data bases, you get a row in a dataframe, and a confidence score in the identification. 

Lets try miss-spelling a common name:

```{r}
gnr_resolve("Lynx cramadensis")
```

Wow! So we can even recover incorrectly spelt latin names. Fantastic. 

Let us tun this code into a useful work flow! We will use a confidence threshold of 90% for out designations.


```{r}
# First add a column to the species list with genus and species pasted together
sp_list$sp <- paste(sp_list$genus, sp_list$species)

# Add a column which states if it is in the databases
sp_list$verified <- NA

# Check if it is in a bunch of different taxonomic databases
for(i in 1:nrow(sp_list))
{
  tmp <- gnr_resolve(sp_list$sp[i])
  
  if(nrow(tmp[tmp$score>0.9,])>0)
  {
    sp_list$verified[i] <- TRUE
  } else{sp_list$verified[i] <- FALSE}
}

sp_list

# Update the row names
row.names(sp_list) <- 1:nrow(sp_list)

```

Lets take another look at that list:

```{r}
sp_list %>%
  kbl(row.names=F) %>%
  kable_styling(full_width = T) 

```

Most of our data was verified - great! But what about those common names. Well there is a way we can get those too using `sci2comm()`"

```{r}
sci2comm("Lynx canadensis")
```

Lets run it through all of our data (as above):

```{r}
# First add a column to the species list with genus and species pasted together
sp_list$sp <- paste(sp_list$genus, sp_list$species)

# Check if it is in a bunch of different taxonomic databases
for(i in 1:nrow(sp_list))
{
  tmp <- sci2comm(sp_list$sp[i])
  
  if(length(tmp[[1]])>0)
  {
    sp_list$common_name[i] <- tmp[[1]]
  } 
}

```

Let's take another look: 

```{r}
sp_list %>%
  kbl(row.names=F) %>%
  kable_styling(full_width = T) 

```

Not bad - let's manually update the reamining species. It is definately far better than googling all of them!

```{r}
sp_list$common_name[sp_list$sp=="Perisoreus canadensis"] <- "Canada jay"
sp_list$common_name[sp_list$sp=="Colaptes auratus"] <- "northern flicker"
sp_list$common_name[sp_list$sp=="Cervus canadensis"] <- "elk"

```

Lets export this list right now, we will return to it later to add other traits which we may use in analysis. 


Then let's write our species list into it:

```{r}
write.csv(sp_list, "data/raw_data/raw_species_list.csv")
```

**making corrections** DO A SECTION ON THIS???



#### Removing non-target species

For the sake of this course, we will subset this data to just be mammals which can be identified to spcies level. This will be the dataset we use moving forward. Note, you may not want to do this everytime. There may be scenarions when you want to include all species in your analyses. 

```{r}
focal_sp_list <- sp_list[sp_list$class=="Mammalia" & sp_list$verified==TRUE,]

```



First lets create a directory to save our 'processed' data!

```{r}
dir.create("data/processed_data")
```





**Raw camera detections**

Raw detections represent the number of images which have a certain species label. Basic summaries about number of images of each species are often required for reports. 

```{r captures, echo=F, fig.height=sp.height}
layout(matrix(c(1,1,2), 1, 3, byrow = TRUE))
det.sum.total <- as.data.frame(count(dat[dat$Blank==FALSE & is.na(dat$Species)==FALSE,], Species))
det.sum.total <- det.sum.total[order(det.sum.total$n),]

par(mar=c(5,16,1,1))
barplot(det.sum.total$n, names.arg = paste0(det.sum.total$Species, 
                                           " (n =", det.sum.total$n,")")   , las=1, cex.names=1, xlab="Total detections", horiz=T)
i <-1
for(i in 1:nrow(det.sum.total))
{
  tmp <- subset(dat, Species==det.sum.total$Species[i])
  det.sum.total$Locations[i] <- length(unique(tmp$deployment_id))
}
par(mar=c(5,1,1,1))

barplot(det.sum.total$Locations/n.stat, las=1, cex.names=0.7, xlab="Proportion of sites detected", horiz=T, xlim=c(0,1))
abline(v=1, lty=2)

```

### Detection check
The following plot helps you determine if you have detections occurring outside of the times cameras are active. *Important note* You can still get detections outside of the activity period if you have decided that the field of view was shifted and the data is uncompariable to that which was collected earlier.  

```{r, include=F}
# Make species colour codes
tmp3 <- data.frame("Species"=unique(dat$Species),"Colour"= turbo(length(unique(dat$Species))))

```


```{r detecion summary, echo=F, message=F, warning=F}

# Make a separate plot for each 20 stations For each 20 stations
# To do this make a plot dattaframe
tmp4 <- data.frame("deployment_id"=plot.order, "Plot.grp"=ceiling(1:length(unique(dep$deployment_id))/20))

dep <- left_join(dep,tmp4, by="deployment_id")
i<- 1
j <- 1
for(j in 1:length(unique(dep$Plot.grp)))
{
    layout(matrix(c(1,1,1,2), 1, 4, byrow = TRUE))
    par(mar=c(2,6,1,1))
    
    plot(c(min(dep$start_date, na.rm=T), max(dep$end_date, na.rm=T)),      c(1,length(unique(dep$deployment_id[dep$Plot.grp==j]))), las=1, ylab="", xlab="", type="n", yaxt="n")
    
    axis(2, at= 1:length(unique(dep$deployment_id[dep$Plot.grp==j])), labels= unique(dep$deployment_id[dep$Plot.grp==j]), las=1, cex.axis=1)
    #mtext("Camera Deployment ID", 2, 4)
    # Make lines for each of the cameras
    for(i in 1:length(unique(dep$deployment_id[dep$Plot.grp==j])))
    {
      abline(h=i, col=rgb(0,0,0,0.1))
      tmp <- dep[dep$deployment_id==unique(dep$deployment_id[dep$Plot.grp==j])[i],]
      
      tmp2 <- dat[dat$deployment_id==tmp$deployment_id[1],]
      tmp2 <- left_join(tmp2, tmp3, Joining, by = "Species")
      points(tmp2$Date_Time.Captured, rep(i,nrow(tmp2)), pch="|", col= tmp2$Colour)
    
      for(k in 1:nrow(tmp))
        {
          lines(c(tmp$start_date[k],
                           tmp$end_date[k]),
                c(i,i), lwd=2)
        }
      }
    par(mar=c(0,0,1,0))
    plot.new()
    legend("topleft", legend=tmp3$Species, fill=tmp3$Colour, xpd=TRUE, cex=1.1 )

}

```

### Species metadata
As you progress with image labeling, it is important to check that the additional information you are collecting is consistent across species. In our case, we often try to record the sex, age class and behavior of detected wildlife (where identifiable).

Of the images classified as containing animals, the proportion of photographs assigned to the following categories are as follows:

**Sex**
```{r sex, echo=F, include=F}
col.name <- "Sex"

plot<-FALSE
if(length(colnames(dat)[colnames(dat)==col.name]>0))
   {
      tmp <- table(dat[,col.name][dat$Blank==FALSE], as.character(dat$Species[dat$Blank==FALSE]))
      tmp <- as.data.frame.matrix(tmp)
      
      dat[,col.name]<- factor(dat[,col.name])
      cols <- turbo( length(levels(dat[,col.name])))
      # Name catagories with no data N\A for NOT ASSESSED
      row.names(tmp)[row.names(tmp)==""] <- "N/A"
      # make it the last level
      tmp <- tmp[c(2:nrow(tmp),1),]
      
      data_percentage <- apply(tmp, 2, function(x){x*100/sum(x,na.rm=T)})
      plot<-TRUE
  }

```


```{r sex plot, echo=F, fig.height=sp.height}
if(plot==TRUE)
{
layout(matrix(c(1,1,1,2), 1, 4, byrow = TRUE))
par(mar=c(5,10,1,1))
barplot(data_percentage , border="white",col= cols, ylab="", las=1, xlab="% of observations", cex.names=0.7, horiz=2)
par(mar=c(0,0,4,0))
plot.new()
legend("topleft", legend=row.names(tmp), fill=cols, xpd=TRUE, cex=1.1 )
} else { print('Not included') }
```

**Age**

```{r age, echo=F, include=F}
col.name <- "Age"

plot<-FALSE
if(length(colnames(dat)[colnames(dat)==col.name]>0))
   {
      tmp <- table(dat[,col.name][dat$Blank==FALSE], as.character(dat$Species[dat$Blank==FALSE]))
      tmp <- as.data.frame.matrix(tmp)
      
      dat[,col.name]<- factor(dat[,col.name])
      cols <- turbo( length(levels(dat[,col.name])))
      # Name catagories with no data N\A for NOT ASSESSED
      row.names(tmp)[row.names(tmp)==""] <- "N/A"
      # make it the last level
      tmp <- tmp[c(2:nrow(tmp),1),]
      
      data_percentage <- apply(tmp, 2, function(x){x*100/sum(x,na.rm=T)})
      plot<-TRUE
  }

```


```{r age plot, echo=F, fig.height=sp.height}
if(plot==TRUE)
{
layout(matrix(c(1,1,1,2), 1, 4, byrow = TRUE))
par(mar=c(5,10,1,1))
barplot(data_percentage , border="white",col= cols, ylab="", las=1, xlab="% of observations", cex.names=0.7, horiz=2)
par(mar=c(0,0,4,0))
plot.new()
legend("topleft", legend=row.names(tmp), fill=cols, xpd=TRUE, cex=1.1 )
} else { print('Not included') }

```

**Behaviour**

```{r Behaviour, echo=F, include=F}
col.name <- "Behaviour"
plot<-FALSE
if(length(colnames(dat)[colnames(dat)==col.name]>0))
   {
      tmp <- table(dat[,col.name][dat$Blank==FALSE], as.character(dat$Species[dat$Blank==FALSE]))
      tmp <- as.data.frame.matrix(tmp)
      
      dat[,col.name]<- factor(dat[,col.name])
      cols <- turbo( length(levels(dat[,col.name])))
      # Name catagories with no data N\A for NOT ASSESSED
      row.names(tmp)[row.names(tmp)==""] <- "N/A"
      # make it the last level
      tmp <- tmp[c(2:nrow(tmp),1),]
      
      data_percentage <- apply(tmp, 2, function(x){x*100/sum(x,na.rm=T)})
      plot<-TRUE
  }
```


```{r behaviour plot, echo=F, fig.height=sp.height}
if(plot==TRUE)
{
layout(matrix(c(1,1,1,2), 1, 4, byrow = TRUE))
par(mar=c(5,10,1,1))
barplot(data_percentage , border="white",col= cols, ylab="", las=1, xlab="% of observations", cex.names=0.7, horiz=2)
par(mar=c(0,0,4,0))
plot.new()
legend("topleft", legend=row.names(tmp), fill=cols, xpd=TRUE, cex=1.1 )
} else { print('Not included') }
```

### Independent camera detections
We rarely analyse raw camera data, rather we filter out multiple detections of the same individual within a given event. This is called creating and "independent detections" dataframe. 


```{r}
# Set the "independence" interval in minutes
independent <- 30

```



```{r indepedents, echo=F, eval=T, message = F, warning = F}
# Remove onservations without animals detected
dat <- dat[dat$Blank==FALSE & is.na(dat$Species)==FALSE,]
dat$Species <- as.character(dat$Species)
dat$deployment_id <- as.character(dat$deployment_id)

# Order the datframe by Site, date
dat <- dat[order(dat$deployment_id, img$timestamp),]


  dat <- dat %>%
  #filter(Species == i) %>%
  arrange(Project.ID,deployment_id) %>%
  group_by(deployment_id, Species) %>%
  mutate(duration = int_length(Date_Time.Captured %--% lag(Date_Time.Captured)))

# loop that assign group ID
dat$Event.ID <- 9999
  mins <- independent
  seq <- as.numeric(paste0(nrow(dat),0))
  seq <- round(seq,-(nchar(seq)))
for (i in 2:nrow(dat)) {
  dat$Event.ID[i-1]  <- paste0("E",format(seq, scientific = F))
  if(is.na(dat$duration[i]) | abs(dat$duration[i]) > (mins * 60)){
    seq <- seq + 1
  }
}

# Update the information for the last row
    # group ID  for the last row
 if(dat$duration[nrow(dat)] < (mins * 60)|
    is.na(dat$duration[nrow(dat)])){
   dat$Event.ID[nrow(dat)] <- dat$Event.ID[nrow(dat)-1]
 } else{
   dat$Event.ID[nrow(dat)] <- paste0("E",format(seq+1, scientific = F))
 }

# If there is no minimum groupsize take number of animals
if(!"Minimum.Group.Size" %in% colnames(dat)) {dat$Minimum.Group.Size <- dat$Number.of.Animals}

# Calculate the event length and size

  # find out the last and the first of the time in the group
  top <- dat %>% group_by(Event.ID) %>% top_n(1,Date_Time.Captured) %>% select(Event.ID, Date_Time.Captured)
  bot <- dat %>% group_by(Event.ID) %>% top_n(-1,Date_Time.Captured) %>% select(Event.ID, Date_Time.Captured)
  names(bot)[2] <- c("Date_Time.Captured_end")
  dec_no <- dat %>% group_by(Event.ID) %>% summarise(n())
  event_grp <- dat %>% group_by(Event.ID) %>% summarise(max(Minimum.Group.Size))

  # caculate the duration
  diff <-  top %>% left_join(bot, by="Event.ID") %>%
      mutate(duration=abs(int_length(Date_Time.Captured %--% Date_Time.Captured_end))) %>%
      left_join(event_grp, by="Event.ID")%>%
      left_join(dec_no, by="Event.ID")

  # Remove duplicates
  diff <- diff[duplicated(diff)==FALSE,]

  names(diff) <- c("Event.ID","Date_Time.end","Date_Time.start","Event.Duration","Event.Groupsize","Event.Observations")
  diff$Date_Time.end<-NULL;diff$Date_Time.start<-NULL
  dat$duration <-NULL
  # Merge the data
  dat <-  dat %>%
   left_join(diff,by="Event.ID")

# Subset to the first observation in each event

  # Subset to independent observations using your chosen threshold
ind.dat <- dat[!duplicated(dat$Event.ID),]
ind.dat <- as.data.frame(ind.dat)
ind.dat$Species <-as.factor(ind.dat$Species)


# Remove all observations with occur outside of camera activity schedules
# We need to know how many detections there are in each month -> create a row lookup
# This is just a list of ever day a camera was active.

tmp <- dep[is.na(dep$end_date)==F,]
daily.lookup <- list()
for(i in 1:nrow(tmp))
{
  if(as.Date(tmp$start_date[i])!=as.Date(tmp$end_date[i]))
  {
    daily.lookup[[i]] <- data.frame("Date"=seq(as.Date(tmp$start_date[i]), as.Date(tmp$end_date[i]), by="days"), "deployment_id"=tmp$deployment_id[i])
  }
}
row.lookup <- do.call(rbind, daily.lookup)

# Remove duplicates
row.lookup <- row.lookup[duplicated(row.lookup)==F,]


# Make a dat/location lookup
tmp <- row.lookup
tmp <- paste(tmp$Date, tmp$deployment_id)

#Subset ind.dat to data that only occurs when a camera is active
ind.dat <- ind.dat[paste(as.Date(ind.img$timestamp), ind.dat$deployment_id) %in% tmp, ]
# Reset the factor levels
ind.dat$Species <- factor(ind.dat$Species)

# Save it for a rainy day
write.csv(ind.dat, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent.csv"), row.names = F)

# Also export the deport lookup
write.csv(row.lookup, paste0("data/processed_data/",dat$Project.ID[1], "_daily_deport_lookup.csv"), row.names = F)

```

Using an independence threshold of `r independent` minutes, the number of detections is reduced to `r nrow(ind.dat)`. The rest of the analyses are conducted with this data. The summary of detections is as follows:

```{r ind captures, echo=F, fig.height=sp.height, eval=T}

layout(matrix(c(1,1,2), 1, 3, byrow = TRUE))
det.sum.total <- as.data.frame(count(ind.dat[ind.dat$Blank==FALSE,], Species))
det.sum.total <- det.sum.total[order(det.sum.total$n),]

par(mar=c(5,16,1,1))
barplot(det.sum.total$n, names.arg = paste0(det.sum.total$Species,
                                           " (n =", det.sum.total$n,")"), las=1, cex.names=1, xlab="Total detections", horiz=T)
i <-1
for(i in 1:nrow(det.sum.total))
{
  tmp <- subset(ind.dat, Species==det.sum.total$Species[i])
  det.sum.total$Locations[i] <- length(unique(tmp$deployment_id))
}
par(mar=c(5,1,1,1))

barplot(det.sum.total$Locations/n.stat, las=1, cex.names=0.7, xlab="Proportion of sites detected", horiz=T, xlim=c(0,1))
abline(v=1, lty=2)

```

** Group size distribution**

```{r group size, echo=F, eval=T,fig.height=sp.height}
par(mfrow=c(1,1))
par(mar=c(5,10,1,1))
plot(jitter(as.numeric(ind.dat$Species))~jitter(ind.dat$Minimum.Group.Size), xlab="Minimum group size", yaxt="n", las=1, ylab="")
axis(2, 1:length(unique(ind.dat$Species)), labels=levels(ind.dat$Species), las=2, cex.axis=0.6)

```


**Site-level species covariance**

This plot shows the co variance between different species at the site level for species with >5 unique detections. For example, if you typically get lots of caribou and bears at the same site, they will have positive co variance. If you get caribou where you don't get bears, they will have negative co variance.

```{r covariance, echo=F, fig.height=sp.height,fig.width=sp.height, eval=T}
par(mfrow=c(1,1))
tmp <- as.data.frame.matrix(table(ind.dat$deployment_id, ind.dat$Species))
tmp <- tmp[colSums(tmp)>5]
M <- cor(tmp)

corrplot(M, method="color", #col=matrix(col(200)),
         type="upper", order="hclust",
         #addCoef.col = "black", # Add codepicient of correlation
         tl.col="black", tl.srt=45, #Text label color and rotation
         # Combine with significance
         #p.mat = p.mat, sig.level = 0.01, insig = "blank",
         # hide correlation codepicient on the principal diagonal
         diag=FALSE
         )

```


### Detection rates
We calculate the detection rates of species to make site-level temporal trends comparable through time where sampling depect is not constant (a common issue in camera trap data sets).

Note, when calculating relative abundance, we use the minimum group size column. 
```{r relative abundance calc, echo=F, warning=F, message=F, eval=T}

det.sum.site <- as.data.frame(table(ind.dat$deployment_id, ind.dat$Species))
colnames(det.sum.site) <- c("deployment_id","Species", "Detections")
det.sum.site$Individuals <- NA

i <- 1
for(i in 1:nrow(det.sum.site))
{
   tmp <- subset(ind.dat, deployment_id==as.character(det.sum.site$deployment_id)[i] &
              Species==as.character(det.sum.site$Species)[i])
   det.sum.site$Individuals[i] <- sum(tmp$Minimum.Group.Size, na.rm=T)
}

# Join with the station deport
CR.site <- left_join(det.sum.site,aggregate(days~deployment_id, data=dep,  FUN=sum, na.rm=T) )
CR.site$CR.100 <- round((CR.site$Individuals/CR.site$days)*100,3)
# Add station locations
CR.site <- left_join(CR.site, sta[, c("deployment_id", "latitude", "longitude")])

```


** Site-level temporal plots **

Across all sites and species:

```{r, echo=F, eval=T}
# Capture rates through time
focal.sp <- as.character(det.sum.total[det.sum.total$n>0,]$Species)
focal.sp <- focal.sp[order(focal.sp)]
# Remove any blanks
focal.sp <- focal.sp[focal.sp!=""]

# Now determine capture rates using the row.lookup
# Make a data frame by month and year
mon.dat <- unique(substr(ind.img$timestamp, 1,7))
mon.dat <- data.frame("Month"=mon.dat[order(mon.dat)], "deport"= NA)
mon.dat[as.character(focal.sp)] <- NA
i<-1
for(i in 1:nrow(mon.dat))
{
  mon.dat$deport[i] <- nrow(subset(row.lookup, substr(row.lookup$Date,1,7)==mon.dat$Month[i]))
  mon.dat$Total.CR[i] <- (nrow(subset(ind.dat, substr(ind.img$timestamp,1,7)==mon.dat$Month[i]))/mon.dat$deport[i])*100
}

for(i in 1:length(focal.sp))
{
  for(j in 1:nrow(mon.dat))
  {
    tmp <- subset(ind.dat, Species==as.character(focal.sp)[i] & substr(ind.img$timestamp,1,7)==mon.dat$Month[j])
    mon.dat[j, as.character(focal.sp[i])] <- (nrow(tmp)/mon.dat$deport[j])*100
  }
}

mon.dat$timestamp <- strptime(paste0(as.character(mon.dat$Month),"-15"), "%Y-%m-%d")

# Remove any silly values 
mon.dat <- mon.dat[is.infinite(mon.dat$Total.CR)==F,]

```


```{r overall CR, echo=F, fig.height=4, eval=T}

par(mfrow=c(1,2))

plot(mon.dat$timestamp, mon.dat$deport, ylab="Monthly deport (days)", xlab="Date", type="l", las=1)
points(mon.dat$timestamp, mon.dat$deport, pch=19, col=rgb(0,0,0,0.4))

# Overall capture rate
plot(mon.dat$timestamp, mon.dat$Total.CR, ylab="Monthly total CR per 100 days", xlab="Date", type="l", las=1, ylim=c(0, max(mon.dat$Total.CR)))
points(mon.dat$timestamp, mon.dat$Total.CR, pch=19, col=rgb(0,0,0,0.4))

```

** Species-specific temporal trends**

Species level variation in monthly capture rates are as follows:

```{r, echo=F, eval=T}
par(mfrow=c(2,3))
for(i in 1:length(focal.sp))
{
  plot(mon.dat$timestamp, mon.dat[,as.character(focal.sp)[i]], ylab="Capture Rate per 100 days", xlab="", type="l", las=1, main=focal.sp[i])
  points(mon.dat$timestamp, mon.dat[,as.character(focal.sp)[i]], pch=19, col=rgb(0,0,0,0.4))
}

```

### Data exporting for analysis
Finally, this script outputs 10 useful data frames for future data analysis:

```{r}
# Create an ouput folder 
dir.create("processed_data/")

```




1. A data frame of "independent detections" at the `r independent` minute threshold you specified at the start: 

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent.csv")`"
  
2. The "deport lookup" which is a dataframe of all days a given camera station was active. Some people use an deport matrix for this step, but we find the long format is much easier to use in downstream analysis. 
  - "`r paste0("data/processed_data/",dat$Project.ID[1], "_daily_deport_lookup.csv")`"  

3 & 4: A 'site x species' matrix of the number of independent detections and species counts across the full study period:

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_total_observations.csv")`"

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_total_counts.csv")`"


5 & 6: A 'site_month x species' matrix of the number of independent detections and species counts across for each month in the study period:

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Monthly_total_observations.csv")`"

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Monthly_total_counts.csv")`"


7 & 8: A 'site_week x species' matrix of the number of independent detections and species counts across for each week in the study period:

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Weekly_total_observations.csv")`"

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Weekly_total_counts.csv")`"
  
9 & 10: A 'site_day x species' matrix of the number of independent detections and species counts across for each day a station was active in the study period:

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Daily_total_observations.csv")`"

  - "`r  paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Daily_total_counts.csv")`"
  

```{r, echo=F}
# Only do this for species you are interested in
ind.dat <- ind.dat[!ind.dat$Species %in% c("", "No Animal"),]
ind.dat$Species <- factor(ind.dat$Species)
#levels(ind.dat$Species)
```

```{r, echo=F, message=F, warning=F}
# Total counts
  # Station / Month / deport / Species      
  tmp <- row.lookup
  
  # Calculate the number of days at each site  
  total.obs <- tmp %>% 
      group_by(deployment_id) %>%
      summarise(deport = n())
  # Convert to a data frame
  total.obs <- as.data.frame(total.obs)
  
  # Add columns for each species  
  total.obs[, levels(ind.dat$Species)] <- NA
  # Duplicate for counts
  total.count <- total.obs
  # Test counter
  i <-1
  # For each station, count the number of individuals/observations
  for(i in 1:nrow(total.obs))
    {
      tmp <- ind.dat[ind.dat$deployment_id==total.obs$deployment_id[i],]
      
      for(j in 1:length(levels(ind.dat$Species)))
      {
        total.obs[i,levels(ind.dat$Species)[j]] <- length(tmp$Species[tmp$Species==levels(ind.dat$Species)[j]])
        total.count[i,levels(ind.dat$Species)[j]] <- sum(tmp$Event.Groupsize[tmp$Species==levels(ind.dat$Species)[j]])
      }
    }

#write.csv(total.obs, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_total_observations.csv"), row.names = F) 

#write.csv(total.count, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_total_counts.csv"), row.names = F) 

```

```{r, echo=F, message=F, warning=F}

# Monthly counts
  # Station / Month / deport / Covariates / Species      
  tmp <- row.lookup
  # Simplify the date to monthly
  tmp$Date <- substr(tmp$Date,1,7)
  
  # Calculate the number of days in each month  
  mon.obs <- tmp %>% 
      group_by(deployment_id,Date ) %>%
      summarise(deport = n())
  # Convert to a data frame
  mon.obs <- as.data.frame(mon.obs)
    
  mon.obs[, levels(ind.dat$Species)] <- NA
  mon.count <- mon.obs
  # For each month, count the number of individuals/observations
  for(i in 1:nrow(mon.obs))
    {
      tmp <- ind.dat[ind.dat$deployment_id==mon.obs$deployment_id[i] & substr(ind.img$timestamp,1,7)== mon.obs$Date[i],]
      for(j in 1:length(levels(ind.dat$Species)))
      {
        mon.obs[i,levels(ind.dat$Species)[j]] <- length(tmp$Species[tmp$Species==levels(ind.dat$Species)[j]])
        mon.count[i,levels(ind.dat$Species)[j]] <- sum(tmp$Event.Groupsize[tmp$Species==levels(ind.dat$Species)[j]])
      }
    }

  
#write.csv(mon.obs, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_Monthly_observations.csv"), row.names = F) 

#write.csv(mon.count, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_Monthly_counts.csv"), row.names = F) 

```


```{r, echo=F, message=F, warning=F}
# Weekly format
  # Station / Month / deport / Covariates / Species      
  tmp <- row.lookup
  # Simplify the date to year-week
  tmp$Date <- strftime(tmp$Date, format = "%Y-W%U")
  # The way this is coded is the counter W01 starts at the first sunday of the year, everything before that is W00. Weeks do not roll accross years.
  
  # Calculate the number of days in each week  
  week.obs <- tmp %>% 
      group_by(deployment_id,Date ) %>%
      summarise(deport = n())
  
  # Convert to a data frame
  week.obs <- as.data.frame(week.obs)
  # Add species columns  
  week.obs[, levels(ind.dat$Species)] <- NA
  week.count <- week.obs
  # For each week, count the number of individuals/observations
  for(i in 1:nrow(week.obs))
    {
      tmp <- ind.dat[ind.dat$deployment_id==week.obs$deployment_id[i] & strftime(ind.img$timestamp, format = "%Y-W%U")== week.obs$Date[i],]
      
      
      
      for(j in 1:length(levels(ind.dat$Species)))
      {
        week.obs[i,levels(ind.dat$Species)[j]] <- length(tmp$Species[tmp$Species==levels(ind.dat$Species)[j]])
        week.count[i,levels(ind.dat$Species)[j]] <- sum(tmp$Event.Groupsize[tmp$Species==levels(ind.dat$Species)[j]])
      }
    }
#write.csv(week.obs, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_weekly_observations.csv"), row.names = F) 

#write.csv(week.count, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_weekly_counts.csv"), row.names = F) 

```


```{r, echo=F, message=F, warning=F}
# Daily format
  # Station / Month / deport / Covariates / Species      
  tmp <- row.lookup
  tmp$deport <- 1
  # Add species columns  
  tmp[, levels(ind.dat$Species)] <- NA
  
  day.obs <- tmp
  day.count <- tmp
# For each week, count the number of individuals/observations
  for(i in 1:nrow(day.obs))
    {
      tmp <- ind.dat[ind.dat$deployment_id==day.obs$deployment_id[i] & strftime(ind.img$timestamp, format = "%Y-%m-%d")== day.obs$Date[i],]
      
      
      
      for(j in 1:length(levels(ind.dat$Species)))
      {
        day.obs[i,levels(ind.dat$Species)[j]] <- length(tmp$Species[tmp$Species==levels(ind.dat$Species)[j]])
        day.count[i,levels(ind.dat$Species)[j]] <- sum(tmp$Event.Groupsize[tmp$Species==levels(ind.dat$Species)[j]])
      }
    }
#write.csv(day.obs, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_daily_observations.csv"), row.names = F) 

#write.csv(day.count, paste0("data/processed_data/",dat$Project.ID[1], "_",independent ,"min_Independent_daily_counts.csv"), row.names = F) 

```


**Final data check**

Finally, as a last check that our code is creating robust analysis data frames, we check if the observations/counts are the same across each temporal scale (total/monthly/weekly/daily). Check this using the following tables. 

**Observations**
```{r, echo=F}

tmp <- cbind(data.frame("Time"=c("Total", "Monthly", "Weekly", "Daily")),
rbind(colSums(total.obs[,2:ncol(total.obs)]),
colSums(mon.obs[,3:ncol(mon.obs)]),
colSums(week.obs[,3:ncol(week.obs)]),
colSums(day.obs[,3:ncol(day.obs)])  ))

tmp %>%
  kbl() %>%
  kable_styling(full_width = T) %>%
  column_spec(1, bold = T, border_right = T)

```

** Counts **
```{r, echo=F}
tmp <- cbind(data.frame("Time"=c("Total", "Monthly", "Weekly", "Daily")),
rbind(colSums(total.count[,2:ncol(total.count)]),
colSums(mon.count[,3:ncol(mon.count)]),
colSums(week.count[,3:ncol(week.count)]),
colSums(day.count[,3:ncol(day.count)])  ))

tmp %>%
  kbl() %>%
  kable_styling(full_width = T) %>%
  column_spec(1, bold = T, border_right = T)

```


